{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building synchronization state...\n",
      "Starting synchronization\n"
     ]
    }
   ],
   "source": [
    "!gsutil -m rsync -r gs://elvos/multichannel_mip_data/ /home/amy/data/amy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/amy/data/amy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_secret.json    from_numpy  training_labels.csv\r\n",
      "from_luke_training    tmp\t  validation\r\n",
      "from_luke_validation  training\t  validation_labels.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/amy/data/amy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "BLACKLIST = ['LAUIHISOEZIM5ILF',\n",
    "             '2018050121043822',\n",
    "             '2018050120260258']\n",
    "\n",
    "def normalize(image, lower_bound=None, upper_bound=None):\n",
    "    # TODO: This is an issue, we can't zero center per image\n",
    "    if lower_bound is None:\n",
    "        lower_bound = image.min()\n",
    "    if upper_bound is None:\n",
    "        upper_bound = image.max()\n",
    "\n",
    "    image[image > upper_bound] = upper_bound\n",
    "    image[image < lower_bound] = lower_bound\n",
    "\n",
    "    return (image - image.mean()) / image.std()\n",
    "\n",
    "class MipGenerator(object):\n",
    "\n",
    "    def __init__(self, dims=(120, 120, 1), batch_size=16,\n",
    "                 shuffle=True,\n",
    "                 validation=False,\n",
    "                 split=0.2, extend_dims=True,\n",
    "                 augment_data=True):\n",
    "        self.dims = dims\n",
    "        self.batch_size = batch_size\n",
    "        self.extend_dims = extend_dims\n",
    "        self.augment_data = augment_data\n",
    "        self.validation = validation\n",
    "\n",
    "        self.datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "\n",
    "        # Delete all content in tmp/npy/\n",
    "        filelist = [f for f in os.listdir('/home/amy/data/amy1/tmp/npy')]\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join('/home/amy/data/amy1/tmp/npy', f))\n",
    "\n",
    "        # Get npy files from Google Cloud Storage\n",
    "        gcs_client = storage.Client.from_service_account_json(\n",
    "            '/home/amy/data/amy1/client_secret.json'\n",
    "        )\n",
    "        bucket = gcs_client.get_bucket('elvos')\n",
    "        blobs = bucket.list_blobs(prefix='multichannel_mip_data/from_numpy/')\n",
    "\n",
    "        files = []\n",
    "        for blob in blobs:\n",
    "            file = blob.name\n",
    "\n",
    "            # Check blacklist\n",
    "            blacklisted = False\n",
    "            for each in BLACKLIST:\n",
    "                if each in file:\n",
    "                    blacklisted = True\n",
    "\n",
    "            if not blacklisted:\n",
    "                # Add all data augmentation methods\n",
    "                files.append({\n",
    "                    \"name\": file,\n",
    "                })\n",
    "\n",
    "                if self.augment_data and not self.validation:\n",
    "                    self.__add_augmented(files, file)\n",
    "\n",
    "        # Split based on validation\n",
    "        if validation:\n",
    "            files = files[:int(len(files) * split)]\n",
    "        else:\n",
    "            files = files[int(len(files) * split):]\n",
    "\n",
    "        # Get label data from Google Cloud Storage\n",
    "        blob = storage.Blob('labels.csv', bucket)\n",
    "        blob.download_to_filename('/home/amy/data/amy1/tmp/labels.csv')\n",
    "        label_data = {}\n",
    "        with open('/home/amy/data/amy1/tmp/labels.csv', 'r') as pos_file:\n",
    "            reader = csv.reader(pos_file, delimiter=',')\n",
    "            for row in reader:\n",
    "                if row[0] != 'patient_id':\n",
    "                    label_data[row[0]] = int(row[1])\n",
    "\n",
    "        labels = np.zeros(len(files))\n",
    "        for i, file in enumerate(files):\n",
    "            filename = file['name']\n",
    "            filename = filename.split('/')[-1]\n",
    "            filename = filename.split('.')[0]\n",
    "            filename = filename.split('_')[0]\n",
    "            labels[i] = label_data[filename]\n",
    "\n",
    "        # Take into account shuffling\n",
    "        if shuffle:\n",
    "            tmp = list(zip(files, labels))\n",
    "            random.shuffle(tmp)\n",
    "            files, labels = zip(*tmp)\n",
    "            labels = np.array(labels)\n",
    "\n",
    "        self.files = files\n",
    "        self.labels = labels\n",
    "        self.bucket = bucket\n",
    "\n",
    "    def __add_augmented(self, files, file):\n",
    "        for i in range(1):\n",
    "            files.append({\n",
    "                \"name\": file,\n",
    "            })\n",
    "\n",
    "    def generate(self):\n",
    "        steps = self.get_steps_per_epoch()\n",
    "        while True:\n",
    "            for i in range(steps):\n",
    "                #print(i)\n",
    "                x, y = self.__data_generation(i)\n",
    "                yield x, y\n",
    "\n",
    "    def get_steps_per_epoch(self):\n",
    "        return len(self.files) // self.batch_size\n",
    "\n",
    "    def __data_generation(self, i):\n",
    "        bsz = self.batch_size\n",
    "        files = self.files[i * bsz:(i + 1) * bsz]\n",
    "        labels = self.labels[i * bsz:(i + 1) * bsz]\n",
    "        images = []\n",
    "\n",
    "        # Download files to tmp/npy/\n",
    "        for i, file in enumerate(files):\n",
    "            blob = self.bucket.get_blob(file['name'])\n",
    "            file_id = file['name'].split('/')[-1]\n",
    "            file_id = file_id.split('.')[0]\n",
    "            blob.download_to_filename(\n",
    "                '/home/amy/data/amy1/tmp/npy/{}.npy'.format(file_id)\n",
    "            )\n",
    "            img = np.load('/home/amy/data/amy1/tmp/npy/{}.npy'.format(file_id))\n",
    "            os.remove('/home/amy/data/amy1/tmp/npy/{}.npy'.format(file_id))\n",
    "            img = self.__transform_images(img)\n",
    "            # print(np.shape(img))\n",
    "            images.append(img)\n",
    "        images = np.array(images)\n",
    "        return images, labels\n",
    "\n",
    "    def __transform_images(self, image):\n",
    "        image = np.moveaxis(image, 0, -1)\n",
    "\n",
    "        # Set bounds\n",
    "        image[image < -40] = -40\n",
    "        image[image > 400] = 400\n",
    "\n",
    "        # Normalize image and expand dims\n",
    "        image = normalize(image)\n",
    "        if self.extend_dims:\n",
    "            if len(self.dims) == 2:\n",
    "                image = np.expand_dims(image, axis=-1)\n",
    "            else:\n",
    "                image = np.repeat(image[:, :, np.newaxis],\n",
    "                                  self.dims[2], axis=2)\n",
    "\n",
    "        # Data augmentation methods\n",
    "        if self.augment_data and not self.validation:\n",
    "            image = self.datagen.random_transform(image)\n",
    "\n",
    "        # Interpolate axis to reduce to specified dimensions\n",
    "        dims = np.shape(image)\n",
    "        image = zoom(image, (self.dims[0] / dims[0],\n",
    "                             self.dims[1] / dims[1],\n",
    "                             1))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "HELLO 1\n",
      "HELLO 2\n",
      "HELLO 3\n",
      "(?, 208, 208, 1024)\n",
      "HELLO 4\n",
      "(?, 208, 208, 2)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cad_input (InputLayer)          (None, 220, 220, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 220, 220, 96) 7296        cad_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 220, 220, 96) 230496      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 110, 110, 96) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 110, 110, 128 307328      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 110, 110, 128 409728      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 55, 55, 128)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 55, 55, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 55, 55, 256)  590080      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 27, 27, 256)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 27, 27, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 27, 27, 512)  2359808     conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 13, 13, 512)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 13, 13, 1024) 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 13, 13, 1024) 9438208     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 26, 26, 512)  4719104     conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 26, 26, 512)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 26, 26, 1024) 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 cropping2d_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 52, 52, 256)  2359552     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_2 (Cropping2D)       (None, 52, 52, 256)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 52, 52, 512)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 cropping2d_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 104, 104, 128 1638528     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)       (None, 104, 104, 128 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 104, 104, 256 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 cropping2d_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 208, 208, 96) 614496      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_4 (Cropping2D)       (None, 208, 208, 96) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 208, 208, 192 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 cropping2d_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 208, 208, 102 197632      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 208, 208, 102 1049600     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 208, 208, 2)  2050        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,118,850\n",
      "Trainable params: 30,118,850\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input, BatchNormalization,\n",
    "    Dense, Flatten, Conv2DTranspose,\n",
    "    Concatenate, concatenate, Cropping2D\n",
    ")\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "\n",
    "\n",
    "class SimpleUNetBuilder(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def build(input_shape, num_classes=2):\n",
    "        \"\"\"Create a 3D Convolutional Autoencoder model.\n",
    "\n",
    "        Parameters:\n",
    "        - input_shape: Tuple of input shape in the format\n",
    "            (conv_dim1, conv_dim2, conv_dim3, channels)\n",
    "        - initial_filter: Initial filter size. This will be doubled\n",
    "            for each hidden layer as it goes deeper.\n",
    "        - num_encoding_layers: Number of encoding convolutional +\n",
    "            pooling layers. The number of decoding\n",
    "            layers will be the same.\n",
    "\n",
    "        Returns:\n",
    "        - A 3D CAD model that takes a 5D tensor (volumetric images\n",
    "        in batch) as input and returns a 5D vector (prediction) as output.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(input_shape) != 3:\n",
    "            raise ValueError(\"Input shape should be a tuple \"\n",
    "                             \"(conv_dim1, conv_dim2, conv_dim3)\")\n",
    "\n",
    "        input_img = Input(shape=input_shape, name=\"cad_input\")\n",
    "\n",
    "        # Conv1 (Output n, n, 96)\n",
    "        conv1 = Conv2D(96, (5, 5), activation='relu',\n",
    "                       padding='same')(input_img)\n",
    "        conv1 = Conv2D(96, (5, 5), activation='relu',\n",
    "                       padding='same')(conv1)\n",
    "\n",
    "        # Conv2 (Output n/2, n/2, 128)\n",
    "        conv2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, (5, 5), activation='relu',\n",
    "                       padding='same')(conv2)\n",
    "        conv2 = Conv2D(128, (5, 5), activation='relu',\n",
    "                       padding='same')(conv2)\n",
    "\n",
    "        # Conv3 (Output n/4, n/4, 256)\n",
    "        conv3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv2)\n",
    "        conv3 = Conv2D(256, (3, 3), activation='relu',\n",
    "                       padding='same')(conv3)\n",
    "        conv3 = Conv2D(256, (3, 3), activation='relu',\n",
    "                       padding='same')(conv3)\n",
    "\n",
    "        # Conv4 (Output n/8, n/8, 512)\n",
    "        conv4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv3)\n",
    "        conv4 = Conv2D(512, (3, 3), activation='relu',\n",
    "                       padding='same')(conv4)\n",
    "        conv4 = Conv2D(512, (3, 3), activation='relu',\n",
    "                       padding='same')(conv4)\n",
    "\n",
    "        # Conv5 (Output n/16, n/16, 1024)\n",
    "        conv5 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(conv4)\n",
    "        conv5 = Conv2D(1024, (3, 3), activation='relu',\n",
    "                       padding='same')(conv5)\n",
    "        conv5 = Conv2D(1024, (3, 3), activation='relu',\n",
    "                       padding='same')(conv5)\n",
    "\n",
    "        # begin resizing attempt\n",
    "\n",
    "        # deconv1 (Output n/8, n/8, 512)\n",
    "        deconv1 = Conv2DTranspose(512, (3, 3), strides=(2, 2),\n",
    "                                activation='relu', padding='same')(conv5)\n",
    "        deconv1_1 = Cropping2D(((0, 1), (0, 1)))(conv4)\n",
    "        both_1 = concatenate([deconv1, deconv1_1])\n",
    "\n",
    "        # deconv2 (Output n/4, n/4, 256)\n",
    "        print(type(both_1))\n",
    "        deconv2 = Conv2DTranspose(256, (3, 3), strides=(2, 2),\n",
    "                                activation='relu', padding='same')(both_1)\n",
    "        deconv2_1 = Cropping2D(((0, 3), (0, 3)))(conv3)\n",
    "        both_2 = concatenate([deconv2, deconv2_1])\n",
    "\n",
    "        # deconv3 (Output n/2, n/2, 128)\n",
    "        deconv3 = Conv2DTranspose(128, (5, 5), strides=(2, 2),\n",
    "                                  activation='relu', padding='same')(both_2)\n",
    "        deconv3_1 = Cropping2D(((0, 6), (0, 6)))(conv2)\n",
    "        both_3 = concatenate([deconv3, deconv3_1])\n",
    "\n",
    "        # deconv4 (Output n, n, 96)\n",
    "        deconv4 = Conv2DTranspose(96, (5, 5), strides=(2, 2),\n",
    "                                  activation='relu', padding='same')(both_3)\n",
    "        deconv4_1 = Cropping2D(((0, 12), (0, 12)))(conv1)\n",
    "        both_4 = concatenate([deconv4, deconv4_1])\n",
    "\n",
    "        # Fully connected layers\n",
    "        print(\"HELLO 1\")\n",
    "        dense1 = Dense(1024, activation='relu', use_bias=True)(both_4)\n",
    "        print(\"HELLO 2\")\n",
    "        dense2 = Dense(1024, activation='relu', use_bias=True)(dense1)\n",
    "        print(\"HELLO 3\")\n",
    "        print(dense2.shape)\n",
    "        output_img = Dense(num_classes, activation='sigmoid',\n",
    "                                    use_bias=True)(dense2)\n",
    "        print(\"HELLO 4\")\n",
    "        print(output_img.shape)\n",
    "\n",
    "        # begin unused code\n",
    "        # Conv2 (Output 50 x 50 x 64)\n",
    "        # x = Conv2D(256, (5, 5), activation='relu', padding='same')(x)\n",
    "        # x = BatchNormalization()(x)\n",
    "        # x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "        # # Conv3 (Output 12 x 12 x 96)\n",
    "        # x = Conv2D(512, (3, 3), activation='relu',\n",
    "        #            padding='same')(x)\n",
    "\n",
    "        # # Conv4 (Output 6 x 6 x 128)\n",
    "        # x = Conv2D(512, (3, 3), activation='relu', strides=(2, 2),\n",
    "        #            padding='same')(x)\n",
    "\n",
    "        # # Conv5 (Output 3 x 3 x 128)\n",
    "        # x = Conv2D(1024, (3, 3), activation='relu', strides=(2, 2),\n",
    "        #            padding='same')(x)\n",
    "\n",
    "        # # Flatten\n",
    "        # x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "        # x = Flatten()(x)\n",
    "\n",
    "        # # Fully connected layers\n",
    "        # x = Dense(1024, activation='relu', use_bias=True)(x)\n",
    "        # x = Dense(1024, activation='relu', use_bias=True)(x)\n",
    "        # output_img = Dense(num_classes, activation='sigmoid',\n",
    "        #                    use_bias=True)(x)\n",
    "\n",
    "        model = Model(inputs=input_img, outputs=output_img)\n",
    "        return model\n",
    "\n",
    "\n",
    "m = SimpleUNetBuilder.build((220, 220, 3))\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_9 to have 4 dimensions, but got array with shape (10, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1ff3aae820d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_steps_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_steps_per_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     epochs = 10)\n\u001b[0m",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    190\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/elvo-analysis/venv/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_9 to have 4 dimensions, but got array with shape (10, 1)"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D \n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Generators\n",
    "training_generator = MipGenerator(dims=(220,220,3),\n",
    "                                  extend_dims=False,\n",
    "                                  batch_size=10,\n",
    "                                  augment_data=True)\n",
    "validation_generator = MipGenerator(dims=(220,220,3),\n",
    "                                  extend_dims=False,\n",
    "                                  batch_size=10,\n",
    "                                  augment_data=True,\n",
    "                                  validation=True)\n",
    "\n",
    "m.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08,decay=0.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "m.fit_generator(generator=training_generator.generate(),\n",
    "                    validation_data=validation_generator.generate(),\n",
    "                    steps_per_epoch=training_generator.get_steps_per_epoch(),\n",
    "                    validation_steps=validation_generator.get_steps_per_epoch(),\n",
    "                    epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MipGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5401cb61eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m training_generator = MipGenerator(dims=(220,220,3),\n\u001b[0m\u001b[1;32m     17\u001b[0m                                   \u001b[0mextend_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MipGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Input, BatchNormalization,\n",
    "    Dense, Flatten, Conv2DTranspose,\n",
    "    Concatenate, Cropping2D\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "#partition =  load_training_data() # IDs\n",
    "#labels = load_labels()[1] # Labels\n",
    "\n",
    "# Generators\n",
    "training_generator = MipGenerator(dims=(220,220,3),\n",
    "                                  extend_dims=False,\n",
    "                                  batch_size=10,\n",
    "                                  augment_data=True)\n",
    "validation_generator = MipGenerator(dims=(220,220,3),\n",
    "                                  extend_dims=False,\n",
    "                                  batch_size=10,\n",
    "                                  augment_data=True,\n",
    "                                  validation=True)\n",
    "\n",
    "# Design model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, input_shape=(220,220,3)))\n",
    "model.add(Dense(32, input_shape=(220,200,3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator.generate(),\n",
    "                    validation_data=validation_generator.generate(),\n",
    "                    steps_per_epoch=training_generator.get_steps_per_epoch(),\n",
    "                    validation_steps=validation_generator.get_steps_per_epoch(),\n",
    "                    epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amy/elvo-analysis/venv/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 408s 3s/step - loss: 0.7747 - acc: 0.5221 - val_loss: 0.6068 - val_acc: 0.6706\n",
      "Epoch 2/15\n",
      "140/140 [==============================] - 369s 3s/step - loss: 0.6925 - acc: 0.5807 - val_loss: 0.5794 - val_acc: 0.6588\n",
      "Epoch 3/15\n",
      "140/140 [==============================] - 370s 3s/step - loss: 0.6334 - acc: 0.6564 - val_loss: 0.5652 - val_acc: 0.7118\n",
      "Epoch 4/15\n",
      "140/140 [==============================] - 390s 3s/step - loss: 0.6371 - acc: 0.6507 - val_loss: 0.5733 - val_acc: 0.6824\n",
      "Epoch 5/15\n",
      "140/140 [==============================] - 379s 3s/step - loss: 0.5758 - acc: 0.6921 - val_loss: 0.5789 - val_acc: 0.7000\n",
      "Epoch 6/15\n",
      "140/140 [==============================] - 362s 3s/step - loss: 0.5546 - acc: 0.6929 - val_loss: 0.5838 - val_acc: 0.6882\n",
      "Epoch 7/15\n",
      "140/140 [==============================] - 361s 3s/step - loss: 0.5195 - acc: 0.7400 - val_loss: 0.6255 - val_acc: 0.6471\n",
      "Epoch 8/15\n",
      "140/140 [==============================] - 370s 3s/step - loss: 0.5126 - acc: 0.7407 - val_loss: 0.6085 - val_acc: 0.7000\n",
      "Epoch 9/15\n",
      "140/140 [==============================] - 362s 3s/step - loss: 0.4696 - acc: 0.7757 - val_loss: 0.5896 - val_acc: 0.6765\n",
      "Epoch 10/15\n",
      "140/140 [==============================] - 376s 3s/step - loss: 0.4239 - acc: 0.7943 - val_loss: 0.6184 - val_acc: 0.6588\n",
      "Epoch 11/15\n",
      "140/140 [==============================] - 369s 3s/step - loss: 0.4251 - acc: 0.7986 - val_loss: 0.6448 - val_acc: 0.6588\n",
      "Epoch 12/15\n",
      "140/140 [==============================] - 366s 3s/step - loss: 0.3971 - acc: 0.8121 - val_loss: 0.6533 - val_acc: 0.6588\n",
      "Epoch 13/15\n",
      "140/140 [==============================] - 361s 3s/step - loss: 0.3568 - acc: 0.8279 - val_loss: 0.6848 - val_acc: 0.6706\n",
      "Epoch 14/15\n",
      "140/140 [==============================] - 359s 3s/step - loss: 0.3437 - acc: 0.8464 - val_loss: 0.6521 - val_acc: 0.6824\n",
      "Epoch 15/15\n",
      "140/140 [==============================] - 375s 3s/step - loss: 0.3169 - acc: 0.8550 - val_loss: 0.6884 - val_acc: 0.6706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe59ff45f98>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D \n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Generators\n",
    "training_generator = MipGenerator(dims=(230,230,3),\n",
    "                                  extend_dims=False,\n",
    "                                  batch_size=10,\n",
    "                                  augment_data=True)\n",
    "validation_generator = MipGenerator(dims=(230,230,3),\n",
    "                                  extend_dims=False,\n",
    "                                  batch_size=10,\n",
    "                                  augment_data=True,\n",
    "                                  validation=True)\n",
    "\n",
    "# expected input shape: (160, 160, 3)\n",
    "#base_model = applications.nasnet.NASNetMobile(input_shape=(220, 220, 3), include_top=False, weights='imagenet', input_tensor=None, pooling=None, classes=1000)\n",
    "\n",
    "base_model = applications.densenet.DenseNet121(include_top=False, weights='imagenet', input_tensor=None, input_shape=(230,230,3), pooling=None, classes=1000)\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "# This consists of a global average pooling layer and a fully connected layer with 256 nodes # Then apply dropout and sigmoid activation\n",
    "model_top = Sequential()\n",
    "model_top.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:], data_format=None))\n",
    "model_top.add(Dense(1024, activation='relu'))\n",
    "model_top.add(Dropout(0.7))\n",
    "model_top.add(Dense(1, activation='sigmoid'))\n",
    "model = Model(inputs=base_model.input, outputs=model_top(base_model.output))\n",
    "# Compile model using Adam optimizer with common values and binary cross entropy loss # Use low learning rate (lr) for transfer learning\n",
    "model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08,decay=0.0), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator.generate(),\n",
    "                    validation_data=validation_generator.generate(),\n",
    "                    steps_per_epoch=training_generator.get_steps_per_epoch(),\n",
    "                    validation_steps=validation_generator.get_steps_per_epoch(),\n",
    "                    epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib library, and plot training cuve\n",
    "import matplotlib.pyplot as plt \n",
    "print(history.history.keys())\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'], 'orange', label='Training accuracy') \n",
    "plt.plot(history.history['val_acc'], 'blue', label='Validation accuracy') \n",
    "plt.plot(history.history['loss'], 'red', label='Training loss') \n",
    "plt.plot(history.history['val_loss'], 'green', label='Validation loss') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model-densenet-06-18-18')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
